{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 4: T·ªëi ∆∞u h√≥a MLPClassifier b·∫±ng GridSearchCV\n",
    "\n",
    " M·ª•c ti√™u: t√¨m c·∫•u h√¨nh t·ªët nh·∫•t cho MLP tr√™n t·∫≠p d·ªØ li·ªáu Iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== B∆∞·ªõc 4.1: Ki·ªÉm tra v√† t·∫°o d·ªØ li·ªáu ƒë·∫ßu v√†o ===\n",
      "üìÇ Ch∆∞a c√≥ file iris_scaled.csv ‚Äì ƒêang t·∫°o t·ª´ b·ªô Iris g·ªëc...\n",
      "‚úÖ ƒê√£ l∆∞u iris_scaled.csv (d·ªØ li·ªáu ƒë√£ chu·∫©n h√≥a).\n",
      "üìà Xem th·ª≠ 5 d√≤ng ƒë·∫ßu:\n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0          -0.900681          1.019004          -1.340227         -1.315444   \n",
      "1          -1.143017         -0.131979          -1.340227         -1.315444   \n",
      "2          -1.385353          0.328414          -1.397064         -1.315444   \n",
      "3          -1.506521          0.098217          -1.283389         -1.315444   \n",
      "4          -1.021849          1.249201          -1.340227         -1.315444   \n",
      "\n",
      "   target  \n",
      "0       0  \n",
      "1       0  \n",
      "2       0  \n",
      "3       0  \n",
      "4       0  \n"
     ]
    }
   ],
   "source": [
    "print(\"=== B∆∞·ªõc 4.1: Ki·ªÉm tra v√† t·∫°o d·ªØ li·ªáu ƒë·∫ßu v√†o ===\")\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "if not os.path.exists(\"iris_scaled.csv\"):\n",
    "    print(\"üìÇ Ch∆∞a c√≥ file iris_scaled.csv ‚Äì ƒêang t·∫°o t·ª´ b·ªô Iris g·ªëc...\")\n",
    "    iris = load_iris()\n",
    "    data = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "    data['target'] = iris.target\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(data.drop('target', axis=1))\n",
    "    data_scaled = pd.DataFrame(X_scaled, columns=iris.feature_names)\n",
    "    data_scaled['target'] = data['target']\n",
    "\n",
    "    data_scaled.to_csv('iris_scaled.csv', index=False)\n",
    "    print(\"‚úÖ ƒê√£ l∆∞u iris_scaled.csv (d·ªØ li·ªáu ƒë√£ chu·∫©n h√≥a).\")\n",
    "else:\n",
    "    print(\"‚úÖ ƒê√£ t√¨m th·∫•y iris_scaled.csv.\")\n",
    "\n",
    "print(\"üìà Xem th·ª≠ 5 d√≤ng ƒë·∫ßu:\")\n",
    "print(pd.read_csv(\"iris_scaled.csv\").head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== B∆∞·ªõc 4.2: C·∫•u h√¨nh pipeline v√† tham s·ªë t√¨m ki·∫øm ===\n",
      "üî¢ D·ªØ li·ªáu hu·∫•n luy·ªán: 150 m·∫´u, 4 ƒë·∫∑c tr∆∞ng\n",
      "üîß T·ªïng s·ªë t·ªï h·ª£p si√™u tham s·ªë: 144\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== B∆∞·ªõc 4.2: C·∫•u h√¨nh pipeline v√† tham s·ªë t√¨m ki·∫øm ===\")\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "df = pd.read_csv(\"iris_scaled.csv\")\n",
    "X = df.drop(\"target\", axis=1)\n",
    "y = df[\"target\"]\n",
    "\n",
    "print(f\"üî¢ D·ªØ li·ªáu hu·∫•n luy·ªán: {X.shape[0]} m·∫´u, {X.shape[1]} ƒë·∫∑c tr∆∞ng\")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('mlp', MLPClassifier(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'mlp__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n",
    "    'mlp__activation': ['relu', 'tanh', 'logistic'],\n",
    "    'mlp__solver': ['adam', 'sgd'],\n",
    "    'mlp__alpha': [0.0001, 0.001, 0.01],\n",
    "    'mlp__learning_rate_init': [0.001, 0.01],\n",
    "}\n",
    "\n",
    "total_combinations = (\n",
    "    len(param_grid['mlp__hidden_layer_sizes']) *\n",
    "    len(param_grid['mlp__activation']) *\n",
    "    len(param_grid['mlp__solver']) *\n",
    "    len(param_grid['mlp__alpha']) *\n",
    "    len(param_grid['mlp__learning_rate_init'])\n",
    ")\n",
    "print(f\"üîß T·ªïng s·ªë t·ªï h·ª£p si√™u tham s·ªë: {total_combinations}\")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== B∆∞·ªõc 4.3: Ti·∫øn h√†nh hu·∫•n luy·ªán b·∫±ng GridSearchCV ===\n",
      "‚è≥ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán... (m·ªói d·∫•u 'Fitting' l√† m·ªôt t·ªï h·ª£p tham s·ªë)\n",
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n",
      "‚úÖ Hu·∫•n luy·ªán ho√†n t·∫•t.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== B∆∞·ªõc 4.3: Ti·∫øn h√†nh hu·∫•n luy·ªán b·∫±ng GridSearchCV ===\")\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=cv,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"‚è≥ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán... (m·ªói d·∫•u 'Fitting' l√† m·ªôt t·ªï h·ª£p tham s·ªë)\")\n",
    "grid_search.fit(X, y)\n",
    "print(\"‚úÖ Hu·∫•n luy·ªán ho√†n t·∫•t.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== B∆∞·ªõc 4.4: L∆∞u m√¥ h√¨nh v√† xem k·∫øt qu·∫£ ===\n",
      "üíæ ƒê√£ l∆∞u m√¥ h√¨nh v√† k·∫øt qu·∫£ v√†o 'mlp_gridsearch_model.pkl' v√† 'mlp_gridsearch_results.csv'.\n",
      "\n",
      "üéØ Tham s·ªë t·ªët nh·∫•t t√¨m ƒë∆∞·ª£c:\n",
      "{'mlp__activation': 'tanh', 'mlp__alpha': 0.0001, 'mlp__hidden_layer_sizes': (100, 50), 'mlp__learning_rate_init': 0.001, 'mlp__solver': 'adam'}\n",
      "üèÜ ƒê·ªô ch√≠nh x√°c cao nh·∫•t (cross-validated): 97.33%\n",
      "\n",
      "üîù Top 3 c·∫•u h√¨nh t·ªët nh·∫•t:\n",
      "     mean_test_score                                             params\n",
      "134         0.973333  {'mlp__activation': 'logistic', 'mlp__alpha': ...\n",
      "92          0.973333  {'mlp__activation': 'tanh', 'mlp__alpha': 0.01...\n",
      "106         0.973333  {'mlp__activation': 'logistic', 'mlp__alpha': ...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== B∆∞·ªõc 4.4: L∆∞u m√¥ h√¨nh v√† xem k·∫øt qu·∫£ ===\")\n",
    "joblib.dump(grid_search, \"mlp_gridsearch_model.pkl\")\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "results_df.to_csv(\"mlp_gridsearch_results.csv\", index=False)\n",
    "print(\"üíæ ƒê√£ l∆∞u m√¥ h√¨nh v√† k·∫øt qu·∫£ v√†o 'mlp_gridsearch_model.pkl' v√† 'mlp_gridsearch_results.csv'.\")\n",
    "\n",
    "print(\"\\nüéØ Tham s·ªë t·ªët nh·∫•t t√¨m ƒë∆∞·ª£c:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"üèÜ ƒê·ªô ch√≠nh x√°c cao nh·∫•t (cross-validated): {grid_search.best_score_ * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nüîù Top 3 c·∫•u h√¨nh t·ªët nh·∫•t:\")\n",
    "top3 = results_df.sort_values(by='mean_test_score', ascending=False).head(3)\n",
    "print(top3[['mean_test_score', 'params']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
