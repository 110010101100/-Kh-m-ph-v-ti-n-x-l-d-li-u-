# %%
# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt
import pandas as pd
import numpy as np
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
import seaborn as sns
import matplotlib.pyplot as plt
import joblib


# %%
iris = load_iris()
data = pd.DataFrame(data=iris.data, columns=iris.feature_names)
data['target'] = iris.target  # Th√™m c·ªôt nh√£n (0: Setosa, 1: Versicolor, 2: Virginica)





print("=== B∆∞·ªõc 1.1: Ki·ªÉm tra d·ªØ li·ªáu ban ƒë·∫ßu ===")
print("Th√¥ng tin c∆° b·∫£n v·ªÅ d·ªØ li·ªáu:")
print(data.head())  # Hi·ªÉn th·ªã 5 d√≤ng ƒë·∫ßu
print("\nS·ªë l∆∞·ª£ng m·∫´u v√† ƒë·∫∑c tr∆∞ng:", data.shape)  # (150, 5)
print("\nT√™n c√°c l·ªõp:", iris.target_names)  # ['setosa', 'versicolor', 'virginica']
# Ki·ªÉm tra outlier b·∫±ng IQR (Interquartile Range)
Q1 = data.drop('target', axis=1).quantile(0.25)
Q3 = data.drop('target', axis=1).quantile(0.75)
IQR = Q3 - Q1
outliers = ((data.drop('target', axis=1) < (Q1 - 1.5 * IQR)) | (data.drop('target', axis=1) > (Q3 + 1.5 * IQR))).sum()
print("\nS·ªë l∆∞·ª£ng outlier tr√™n m·ªói ƒë·∫∑c tr∆∞ng:")
print(outliers)


# %%
print("\n=== B∆∞·ªõc 1.2: Ph√¢n t√≠ch d·ªØ li·ªáu ===")
print("Th√¥ng tin c·∫•u tr√∫c d·ªØ li·ªáu:")
print(data.info())  # Ki·ªÉu d·ªØ li·ªáu, s·ªë h√†ng/c·ªôt
print("\nTh·ªëng k√™ m√¥ t·∫£ (mean, std, min, max, ...):")
print(data.describe())  # Mean, std, min, max, percentile
print("\nKi·ªÉm tra gi√° tr·ªã null:")
print(data.isnull().sum())  # Ki·ªÉm tra s·ªë l∆∞·ª£ng gi√° tr·ªã null
print("\nPh√¢n b·ªë c√°c l·ªõp (target):")
print(data['target'].value_counts())  # ƒê·∫øm s·ªë l∆∞·ª£ng m·ªói l·ªõp

# Ki·ªÉm tra outlier b·∫±ng IQR (Interquartile Range)
Q1 = data.drop('target', axis=1).quantile(0.25)
Q3 = data.drop('target', axis=1).quantile(0.75)
IQR = Q3 - Q1
outliers = ((data.drop('target', axis=1) < (Q1 - 1.5 * IQR)) | (data.drop('target', axis=1) > (Q3 + 1.5 * IQR))).sum()
print("\nS·ªë l∆∞·ª£ng outlier tr√™n m·ªói ƒë·∫∑c tr∆∞ng:")
print(outliers)

# %%
print("\n=== B∆∞·ªõc 1.3: Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu ===")
scaler = StandardScaler()  # S·ª≠ d·ª•ng StandardScaler
X = data.drop('target', axis=1)  # L·∫•y c√°c ƒë·∫∑c tr∆∞ng
X_scaled = scaler.fit_transform(X)  # Chu·∫©n h√≥a d·ªØ li·ªáu
data_scaled = pd.DataFrame(X_scaled, columns=iris.feature_names)  # Chuy·ªÉn v·ªÅ DataFrame
data_scaled['target'] = data['target']  # Th√™m l·∫°i c·ªôt nh√£n
print("D·ªØ li·ªáu sau khi chu·∫©n h√≥a (m·∫´u ƒë·∫ßu ti√™n):")
print(data_scaled.head())
print("\nTh·ªëng k√™ sau chu·∫©n h√≥a:")
print(data_scaled.describe())  # Ki·ªÉm tra mean ~0, std ~1

# Ki·ªÉm tra mean v√† std sau chu·∫©n h√≥a
print("\nKi·ªÉm tra mean v√† std sau chu·∫©n h√≥a (ƒë·∫£m b·∫£o mean ~0, std ~1):")
stats = data_scaled.drop('target', axis=1).describe()
print(stats)
print("\nMean c·ªßa c√°c ƒë·∫∑c tr∆∞ng:", stats.loc['mean'].values)
print("Std c·ªßa c√°c ƒë·∫∑c tr∆∞ng:", stats.loc['std'].values)


# L∆∞u scaler v√† d·ªØ li·ªáu chu·∫©n h√≥a
joblib.dump(scaler, 'scaler.pkl')
print("Scaler ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'scaler.pkl'")
data_scaled.to_csv('iris_scaled.csv', index=False)
print("D·ªØ li·ªáu chu·∫©n h√≥a ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'iris_scaled.csv'")

# B∆∞·ªõc 1.4: V·∫Ω Correlation Heatmap
print("\n=== B∆∞·ªõc 1.4: V·∫Ω Correlation Heatmap ===")
plt.figure(figsize=(8, 6))
sns.heatmap(data.corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title("Correlation Heatmap of Iris Dataset")
plt.show()


# %%
print("\n=== B∆∞·ªõc 1.5: V·∫Ω Histograms ===")
data.hist(figsize=(10, 8), bins=20)
plt.suptitle("Histograms of Iris Features")
plt.tight_layout()
plt.show()

# B∆∞·ªõc 1.6: V·∫Ω Boxplot 
print("\n=== B∆∞·ªõc 1.6: V·∫Ω Boxplot (b·ªï sung) ===")
data.drop('target', axis=1).boxplot(figsize=(10, 6))
plt.title("Boxplot of Iris Features")
plt.show()

# %%
print("\n=== B∆∞·ªõc 1.5: V·∫Ω Histograms ===")
data.hist(figsize=(10, 8), bins=20)
plt.suptitle("Histograms of Iris Features")
plt.tight_layout()
plt.show()

# B∆∞·ªõc 1.6: V·∫Ω Boxplot 
print("\n=== B∆∞·ªõc 1.6: V·∫Ω Boxplot (b·ªï sung) ===")
data.drop('target', axis=1).boxplot(figsize=(10, 6))
plt.title("Boxplot of Iris Features")
plt.show()

# %%
print("\n=== B∆∞·ªõc 1.7: Quan s√°t v√† ph√¢n t√≠ch bi·ªÉu ƒë·ªì ===")
print("""
H∆∞·ªõng d·∫´n quan s√°t v√† ph√¢n t√≠ch bi·ªÉu ƒë·ªì:
1. **Correlation Heatmap**:
   - petal length v√† petal width c√≥ t∆∞∆°ng quan cao (~0.96), l√† ƒë·∫∑c tr∆∞ng quan tr·ªçng.
   - sepal length v√† sepal width c√≥ t∆∞∆°ng quan th·∫•p (~0.12).
2. **Histograms**:
   - petal length v√† petal width ph√¢n bi·ªát r√µ Setosa v·ªõi c√°c lo√†i kh√°c.
   - sepal length v√† sepal width c√≥ ph√¢n b·ªë ch·ªìng l·∫•n.
3. **Boxplot**:
   - sepal width c√≥ v√†i outlier, c√°c ƒë·∫∑c tr∆∞ng kh√°c ·ªïn ƒë·ªãnh.
""")

# %%
# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt cho b∆∞·ªõc 2 
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# %%
print("\n=== B∆∞·ªõc 2.1: Chu·∫©n b·ªã d·ªØ li·ªáu ===")
iris = load_iris()
data = pd.DataFrame(data=iris.data, columns=iris.feature_names)
data['target'] = iris.target

scaler = StandardScaler()
X_scaled = scaler.fit_transform(data.drop('target', axis=1))
data_scaled = pd.DataFrame(X_scaled, columns=iris.feature_names)
data_scaled['target'] = data['target']

# %%
print("\n=== B∆∞·ªõc 2.2: Ph√¢n chia d·ªØ li·ªáu Train-Test ===")
X = data_scaled.drop('target', axis=1)
y = data_scaled['target']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
print(f"Train shape: {X_train.shape}, Test shape: {X_test.shape}")

# %%
print("\n=== B∆∞·ªõc 2.3: X√¢y d·ª±ng Baseline model ===")
X_original = data.drop('target', axis=1)
y_original = data['target']

X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(
    X_original, y_original, test_size=0.2, random_state=42, stratify=y_original
)

def baseline_rule(petal_length):
    if petal_length < 2.5:
        return 0
    elif petal_length < 5.0:
        return 1
    else:
        return 2

petal_length_train = X_train_orig.iloc[:, 2].values

manual_predictions = [
    baseline_rule(petal_length) for petal_length in petal_length_train
]
manual_accuracy = np.mean(manual_predictions == y_train_orig.values)

print(f"Baseline model accuracy: {manual_accuracy * 100:.2f}%")

# %%
print("\n=== B∆∞·ªõc 2.4: Hu·∫•n luy·ªán MLPClassifier ===")
model = MLPClassifier(
    hidden_layer_sizes=(100,),
    activation='relu',
    solver='adam',
    learning_rate='constant',
    max_iter=1000,
    random_state=42
)

model.fit(X_train, y_train)
joblib.dump(model, 'iris_mlp_model.pkl')

print(f"Loss sau hu·∫•n luy·ªán: {model.loss_:.4f}")

# %%
print("\n=== B∆∞·ªõc 3.1: Load d·ªØ li·ªáu v√† model ===")
iris = load_iris()
data = pd.DataFrame(data=iris.data, columns=iris.feature_names)
data['target'] = iris.target

scaler = StandardScaler()
X_scaled = scaler.fit_transform(data.drop('target', axis=1))
data_scaled = pd.DataFrame(X_scaled, columns=iris.feature_names)
data_scaled['target'] = data['target']

X = data_scaled.drop('target', axis=1)
y = data_scaled['target']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

model = joblib.load('iris_mlp_model.pkl')

print("\n=== B∆∞·ªõc 3.2: D·ª± ƒëo√°n ===")
y_pred = model.predict(X_test)
print("y_pred:", y_pred)

# %%
print("=== B∆∞·ªõc 3.3: Classification Report ===")
print(classification_report(y_test, y_pred, target_names=iris.target_names))

# %%
print("=== B∆∞·ªõc 3.4: Metrics ===")
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='macro')
recall = recall_score(y_test, y_pred, average='macro')
f1 = f1_score(y_test, y_pred, average='macro')

print(f"Accuracy: {accuracy * 100:.2f}%")
print(f"Precision (macro): {precision * 100:.2f}%")
print(f"Recall (macro): {recall * 100:.2f}%")
print(f"F1-score (macro): {f1 * 100:.2f}%")

# %%
print("\n=== B∆∞·ªõc 3.5: Confusion Matrix ===")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=iris.target_names)
disp.plot(cmap='Blues')
plt.title("Confusion Matrix - MLP Base Model")
plt.show()


#=============================== Buoc 4 ===============================


# step4_mlp_gridsearch.py

print("=== B∆∞·ªõc 4.1: Ki·ªÉm tra v√† t·∫°o d·ªØ li·ªáu ƒë·∫ßu v√†o ===")
import os
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler

if not os.path.exists("iris_scaled.csv"):
    print("üìÇ Ch∆∞a c√≥ file iris_scaled.csv ‚Äì ƒêang t·∫°o t·ª´ b·ªô Iris g·ªëc...")
    iris = load_iris()
    data = pd.DataFrame(data=iris.data, columns=iris.feature_names)
    data['target'] = iris.target

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(data.drop('target', axis=1))
    data_scaled = pd.DataFrame(X_scaled, columns=iris.feature_names)
    data_scaled['target'] = data['target']

    data_scaled.to_csv('iris_scaled.csv', index=False)
    print("‚úÖ ƒê√£ l∆∞u iris_scaled.csv (d·ªØ li·ªáu ƒë√£ chu·∫©n h√≥a).")
else:
    print("‚úÖ ƒê√£ t√¨m th·∫•y iris_scaled.csv.")

print("üìà Xem th·ª≠ 5 d√≤ng ƒë·∫ßu:")
print(pd.read_csv("iris_scaled.csv").head())


print("\n=== B∆∞·ªõc 4.2: C·∫•u h√¨nh pipeline v√† tham s·ªë t√¨m ki·∫øm ===")
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.neural_network import MLPClassifier
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
import joblib

df = pd.read_csv("iris_scaled.csv")
X = df.drop("target", axis=1)
y = df["target"]

print(f"üî¢ D·ªØ li·ªáu hu·∫•n luy·ªán: {X.shape[0]} m·∫´u, {X.shape[1]} ƒë·∫∑c tr∆∞ng")

pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('mlp', MLPClassifier(max_iter=1000, random_state=42))
])

param_grid = {
    'mlp__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],
    'mlp__activation': ['relu', 'tanh', 'logistic'],
    'mlp__solver': ['adam', 'sgd'],
    'mlp__alpha': [0.0001, 0.001, 0.01],
    'mlp__learning_rate_init': [0.001, 0.01],
}

total_combinations = (
    len(param_grid['mlp__hidden_layer_sizes']) *
    len(param_grid['mlp__activation']) *
    len(param_grid['mlp__solver']) *
    len(param_grid['mlp__alpha']) *
    len(param_grid['mlp__learning_rate_init'])
)
print(f"üîß T·ªïng s·ªë t·ªï h·ª£p si√™u tham s·ªë: {total_combinations}")

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)


print("\n=== B∆∞·ªõc 4.3: Ti·∫øn h√†nh hu·∫•n luy·ªán b·∫±ng GridSearchCV ===")
grid_search = GridSearchCV(
    pipeline,
    param_grid,
    cv=cv,
    scoring='accuracy',
    n_jobs=-1,
    verbose=2,
    return_train_score=True
)

print("‚è≥ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán... (m·ªói d·∫•u 'Fitting' l√† m·ªôt t·ªï h·ª£p tham s·ªë)")
grid_search.fit(X, y)
print("‚úÖ Hu·∫•n luy·ªán ho√†n t·∫•t.")


print("\n=== B∆∞·ªõc 4.4: L∆∞u m√¥ h√¨nh v√† xem k·∫øt qu·∫£ ===")
joblib.dump(grid_search, "mlp_gridsearch_model.pkl")
results_df = pd.DataFrame(grid_search.cv_results_)
results_df.to_csv("mlp_gridsearch_results.csv", index=False)
print("üíæ ƒê√£ l∆∞u m√¥ h√¨nh v√† k·∫øt qu·∫£ v√†o 'mlp_gridsearch_model.pkl' v√† 'mlp_gridsearch_results.csv'.")

print("\nüéØ Tham s·ªë t·ªët nh·∫•t t√¨m ƒë∆∞·ª£c:")
print(grid_search.best_params_)
print(f"üèÜ ƒê·ªô ch√≠nh x√°c cao nh·∫•t (cross-validated): {grid_search.best_score_ * 100:.2f}%")

print("\nüîù Top 3 c·∫•u h√¨nh t·ªët nh·∫•t:")
top3 = results_df.sort_values(by='mean_test_score', ascending=False).head(3)
print(top3[['mean_test_score', 'params']])
